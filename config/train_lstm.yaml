# LSTM + Attention model training configuration

input_size: null          # AUTO-DETECTED during loading of processed data
hidden_size: 256
n_layers: 1
n_heads: 4
dropout: 0.037
bidirectional: false
fc_hidden: 128

batch_size: 32
epochs: 20
lr: 0.001

seq_len: 60
